from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.action_chains import ActionChains
import time
from bs4 import BeautifulSoup
import hashlib
from datetime import datetime
from selenium.webdriver.chrome.options import Options
import argparse
import warnings


def clean_text(s):
    return s.replace("\n", " ").replace("\t", " ") if s is not None and len(s) != 0 else ""


# Scrapes all the comments tree structure from SeriousEats recipes
def scrape_comments_tree(all_posts_html, comments_dict = {}, parent_dict = None):
    # Find all the direct comments (sibling comments and not direct replies)
    posts = all_posts_html.findChildren("li", recursive=False)
    # Now process each post
    for post in posts:
        # Create the dictionary object for the current post 
        post_dict = {
            "author": clean_text(post.find("span", {"class": "author"}).text) if post.find("span", {"class": "author"}).text is not None else "",
            "published_date": clean_text(post.find("a", {"class": "time-ago"})['title']) if post.find("a", {"class": "time-ago"}) is not None else "",
            "published_date_timestamp": int(datetime.strptime(post.find("a", {"class": "time-ago"})['title'], '%A, %B %d, %Y %I:%M %p').timestamp()) if post.find("a", {"class": "time-ago"}) is not None else "",
            "comment": clean_text(post.find("div", {"class": "post-message"}).text) if post.find("div", {"class": "post-message"}) is not None else "",
            "likes": post.find("span", {"data-role": "likes"}).text if post.find("span", {"data-role": "likes"}) is not None else "",
            "dislikes": post.find("span", {"data-role": "dislikes"}).text if post.find("span", {"data-role": "dislikes"}) is not None else "",
            "replies": []
        } 

        # We need to check if this node has a parent. If it does we just append this post to the list of replies
        # Otherwise we just append it to the list of parents. 
        # We first need to create a unique code generated by using author + published_date + comment
        dict_key = hashlib.md5(post_dict['author'].encode('utf-8') + post_dict['published_date'].encode('utf-8') + post_dict['comment'].encode('utf-8')).hexdigest()
        # Take half key for efficiency 
        dict_key = dict_key[:len(dict_key) // 2]
        if parent_dict is None: 
            comments_dict[dict_key] = post_dict
        else:
            parent_dict['replies'].append({dict_key: post_dict})
            


        children = post.find("ul", {"data-role": "children"}).findChildren("li", recursive=False)
        # Check if the post has children
        if len(children) != 0:
            # If we found children then we simply run again the method recursively
            scrape_comments_tree(post.find("ul", {"data-role": "children"}), comments_dict, post_dict)



# Function used to extract posts from the Disqus section on SeriousEats
def extract_posts_html(driver):
    # Close the cookies banner
    if popup_tag is not None:
        cookie_banner = driver.find_elements_by_xpath(f"//{popup_tag}[@{popup_attribute}='{popup_attribute_value}']")
        # Check if we can interact with the element
        if(len(cookie_banner) != 0 and cookie_banner[0].is_displayed() and cookie_banner[0].is_enabled()):
            driver.execute_script("arguments[0].click();", cookie_banner[0])

    # Scroll down to the commnets section in order to load comments
    element = driver.find_element_by_xpath(f"//{disqus_wrapper_tag}[@{disqus_wrapper_attribute}='{disqus_wrapper_attribute_value}']")
    actions = ActionChains(driver)
    actions.move_to_element(element).perform()

    # Wait for comments to load
    time.sleep(scraper_delay)
    # Switch to the comments frame
    driver.switch_to.frame(driver.find_element_by_xpath("//iframe[@title='Disqus']"))

    # While there is the load-more button keep loading commnets
    while driver.find_elements_by_xpath("//div[@class='load-more']"):
        try:
            driver.find_elements_by_xpath("//div[@class='load-more']")[0].click()
        except Exception as e:
            # print(e)
            # print("error")
            break
        # Wait to load the comments after pressing the button
        time.sleep(scraper_delay)

    # Get page source soup
    soup = BeautifulSoup(driver.page_source, features="html.parser")
    posts = soup.find("ul", {"class": "post-list"})
    return posts

def load_all_urls(urls_path):
    if urls_path is None or len(urls_path) == 0:
        return []
    
    all_urls = []
    with open(urls_path, 'r') as f:
        for line in f:
            line = line.strip()

            if line != None and len(line) != 0:
                all_urls.append(line)
    return all_urls

if __name__ == '__main__':
    warnings.simplefilter("ignore")

    all_urls = []

    parser = argparse.ArgumentParser(description='Specify parameters for the scraper')
    parser.add_argument('--url_to_scrape', type=str, default=None,
                    help='The url to scrape data from')
    parser.add_argument('--all_urls_path', type=str, default=None,
                    help='The path to the tsv file with a list of all the urls we want to scrape comments from')
    parser.add_argument('--headless', type=str, default="False",
                    help='Whether the scraper should be headless or not')
    parser.add_argument('--scraper_delay', type=int, default=1,
                    help='Delay in seconds for the scraper before executing actions')
    parser.add_argument('--disqus_wrapper_tag', type=str, default=None,
                    help='Wrapper html tag around the disqus iframe')
    parser.add_argument('--disqus_wrapper_attribute', type=str, default=None,
                    help='Wrapper html tag attribute around the disqus iframe')
    parser.add_argument('--disqus_wrapper_attribute_value', type=str, default=None,
                    help='Wrapper html tag attribute value around the disqus iframe')
    parser.add_argument('--popup_tag', type=str, default=None,
                    help='Popup html tag of the element that needs to be clicked to close it')
    parser.add_argument('--popup_attribute', type=str, default=None,
                    help='Popup attribute of the html tag of the element that needs to be clicked to close it (i.e. class)')
    parser.add_argument('--popup_attribute_value', type=str, default=None,
                    help='Popup value of the attribute need to identify the element to click to close the popup')
    args = parser.parse_args()


    # Provide either a single url or a file to scrape from 
    if args.url_to_scrape is not None: 
        url_to_scrape = (args.url_to_scrape).strip()
        all_urls = [url_to_scrape]
    else:
        # If the single url is None then we try to get a list of all the urls to load
        if args.all_urls_path is not None: 
            all_urls_path = (args.all_urls_path).strip()
            all_urls = load_all_urls(all_urls_path)

    headless = True if args.headless == 'True' else False
    scraper_delay = args.scraper_delay

    # If no popup tag is defined then simply ignore the popup
    popup_tag = args.popup_tag
    if args.popup_tag is not None: 
        popup_tag = (args.popup_tag).strip()
        if args.popup_attribute is not None: 
            popup_attribute = (args.popup_attribute).strip()
        if args.popup_attribute_value is not None: 
            popup_attribute_value = args.popup_attribute_value

    # If no popup tag is defined then simply ignore the popup
    disqus_wrapper_tag = args.disqus_wrapper_tag
    if args.disqus_wrapper_tag is not None: 
        disqus_wrapper_tag = (args.disqus_wrapper_tag).strip()
        if args.disqus_wrapper_attribute is not None: 
            disqus_wrapper_attribute = (args.disqus_wrapper_attribute).strip()
        if args.disqus_wrapper_attribute_value is not None: 
            disqus_wrapper_attribute_value = args.disqus_wrapper_attribute_value


    
    if(len(all_urls) == 0):
        print("No urls to scrape")
        exit()


    # Create driver
    chrome_options = Options()
    if headless:
        chrome_options.add_argument("--headless")
    driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)

    for index, url in enumerate(all_urls):

        print(f"Processing url {url} : {index + 1}/{len(all_urls)}")
        driver.get(url)

        try:
            results_dict = {
                "id": hashlib.md5(url.encode('utf-8')).hexdigest(),
                "page_url": url,
                "comments": {}
            }
            posts = extract_posts_html(driver)
            
            comments_dict = {}
            scrape_comments_tree(posts, comments_dict=comments_dict)
            print(f"Comments scraped: {len(comments_dict)}")

            results_dict['comments'] = comments_dict



            # Write Json object only if the recipe has comments
            if len(results_dict['comments']) != 0:
                with open("data/output.json", 'a') as f:
                    f.write(str(results_dict) + '\n')
        except Exception as e:
            print(e)
            continue

    driver.quit()
